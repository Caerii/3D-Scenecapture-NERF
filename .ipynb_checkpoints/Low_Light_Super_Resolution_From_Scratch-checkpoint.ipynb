{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmeKnTlluZCX"
   },
   "source": [
    "\n",
    "Low Light Superresolution using a GAN Architecture\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJhTFzrIjLp2",
    "outputId": "07dc918a-f2d1-4c9b-a89f-f75bc0313309",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alif Jakir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 image pairs successfully.\n",
      "torch.Size([3, 200, 300])\n",
      "torch.Size([3, 400, 600])\n",
      "Train dataset:\n",
      "Low-quality images: 350\n",
      "High-quality images: 350\n",
      "\n",
      "Validation dataset:\n",
      "Low-quality images: 100\n",
      "High-quality images: 100\n",
      "\n",
      "Test dataset:\n",
      "Low-quality images: 50\n",
      "High-quality images: 50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset():\n",
    "    # Define paths\n",
    "    dataset_folder = 'lol_dataset'\n",
    "    low_folder = os.path.join(dataset_folder, 'low')\n",
    "    high_folder = os.path.join(dataset_folder, 'high')\n",
    "\n",
    "    # Sample loading\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    low_quality_imgs = []\n",
    "    high_quality_imgs = []\n",
    "\n",
    "    resize_transform = transforms.Resize((200, 300))  # Resize for low-quality images\n",
    "\n",
    "    for filename in os.listdir(low_folder):\n",
    "        low_qual_img_path = os.path.join(low_folder, filename)\n",
    "        high_qual_img_path = os.path.join(high_folder, filename)\n",
    "\n",
    "        try:\n",
    "            low_qual_img = transform(Image.open(low_qual_img_path))\n",
    "            high_qual_img = transform(Image.open(high_qual_img_path))\n",
    "\n",
    "            low_qual_img = resize_transform(low_qual_img)  # Apply resize transformation for low-quality images\n",
    "\n",
    "            low_quality_imgs.append(low_qual_img)\n",
    "            high_quality_imgs.append(high_qual_img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image pair {filename}: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(low_quality_imgs)} image pairs successfully.\")\n",
    "\n",
    "    if len(low_quality_imgs) != len(high_quality_imgs):\n",
    "        print(\"Error: Incomplete dataset.\")\n",
    "        return None, None\n",
    "\n",
    "    print(low_quality_imgs[0].shape)\n",
    "    print(high_quality_imgs[0].shape)\n",
    "\n",
    "    return low_quality_imgs, high_quality_imgs\n",
    "\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "def split_dataset(low_quality_imgs, high_quality_imgs):\n",
    "    X = list(range(len(low_quality_imgs)))\n",
    "    y = list(range(len(high_quality_imgs)))\n",
    "\n",
    "    # Split into train (70%), validation (20%), test (10%)\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=1/3, random_state=42)\n",
    "\n",
    "    # Create train, validation, and test datasets\n",
    "    train_low_quality_imgs = [low_quality_imgs[i] for i in X_train]\n",
    "    train_high_quality_imgs = [high_quality_imgs[i] for i in y_train]\n",
    "\n",
    "    val_low_quality_imgs = [low_quality_imgs[i] for i in X_val]\n",
    "    val_high_quality_imgs = [high_quality_imgs[i] for i in y_val]\n",
    "\n",
    "    test_low_quality_imgs = [low_quality_imgs[i] for i in X_test]\n",
    "    test_high_quality_imgs = [high_quality_imgs[i] for i in y_test]\n",
    "\n",
    "    return train_low_quality_imgs, train_high_quality_imgs, val_low_quality_imgs, val_high_quality_imgs, test_low_quality_imgs, test_high_quality_imgs\n",
    "\n",
    "num_images = 500\n",
    "\n",
    "class LowLightDataset(Dataset):\n",
    "    def __init__(self, low_quality_imgs, high_quality_imgs):\n",
    "        self.low_quality_imgs = low_quality_imgs\n",
    "        self.high_quality_imgs = high_quality_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_quality_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low_img = self.low_quality_imgs[idx]\n",
    "        high_img = self.high_quality_imgs[idx]\n",
    "        return low_img, high_img\n",
    "\n",
    "# Call the function to load the dataset\n",
    "low_quality_imgs, high_quality_imgs = load_dataset()\n",
    "\n",
    "# Split dataset into train(70%), validation(20%), and test(10%) sets\n",
    "train_low_quality_imgs, train_high_quality_imgs, val_low_quality_imgs, val_high_quality_imgs, test_low_quality_imgs, test_high_quality_imgs = split_dataset(low_quality_imgs, high_quality_imgs)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"Train dataset:\")\n",
    "print(f\"Low-quality images: {len(train_low_quality_imgs)}\")\n",
    "print(f\"High-quality images: {len(train_high_quality_imgs)}\")\n",
    "\n",
    "print(\"\\nValidation dataset:\")\n",
    "print(f\"Low-quality images: {len(val_low_quality_imgs)}\")\n",
    "print(f\"High-quality images: {len(val_high_quality_imgs)}\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "print(f\"Low-quality images: {len(test_low_quality_imgs)}\")\n",
    "print(f\"High-quality images: {len(test_high_quality_imgs)}\")\n",
    "\n",
    "# Create DataLoader objects for train, validation, and test datasets\n",
    "train_dataset = LowLightDataset(train_low_quality_imgs, train_high_quality_imgs)\n",
    "val_dataset = LowLightDataset(val_low_quality_imgs, val_high_quality_imgs)\n",
    "test_dataset = LowLightDataset(test_low_quality_imgs, test_high_quality_imgs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlKZHJl-uh8O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckxgTx7Xuw4a"
   },
   "outputs": [],
   "source": [
    "# Split the datasets into the batches necessary\n",
    "\n",
    "# Split into train (70%), validation (20%), test (10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdgQa5TivKb_",
    "outputId": "2feb9b70-192e-487a-e52e-a5f1e04d8049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWAWaaw54Il3"
   },
   "outputs": [],
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3zj2a2iZuYTf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.tconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.tconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.tconv3 = nn.ConvTranspose2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Skip connections\n",
    "        self.skip1 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "        self.skip2 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "\n",
    "        # Skip connections\n",
    "        s1 = self.skip1(x1)\n",
    "        s2 = self.skip2(x2)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.tconv1(x3)\n",
    "        x = x + s1\n",
    "        x = self.tconv2(x)\n",
    "        x = x + s2\n",
    "        x = self.tconv3(x)\n",
    "\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cYagg0Oukzr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "DataLoaders created.\n",
      "Models loaded.\n",
      "Loss function loaded.\n",
      "Optimizers created.\n",
      "Epoch [0/100]\n",
      "Batch [1/22]\n",
      "Batch [1/22], Generator Loss: 0.708132266998291, Discriminator Loss: 1.3897626399993896\n",
      "Batch [2/22]\n",
      "Batch [2/22], Generator Loss: 0.6980065703392029, Discriminator Loss: 1.3783408403396606\n",
      "Batch [3/22]\n",
      "Batch [3/22], Generator Loss: 0.684342622756958, Discriminator Loss: 1.3774781227111816\n",
      "Batch [4/22]\n",
      "Batch [4/22], Generator Loss: 0.6663116216659546, Discriminator Loss: 1.3830461502075195\n",
      "Batch [5/22]\n",
      "Batch [5/22], Generator Loss: 0.6438714265823364, Discriminator Loss: 1.400578498840332\n",
      "Batch [6/22]\n",
      "Batch [6/22], Generator Loss: 0.6354162693023682, Discriminator Loss: 1.41600501537323\n",
      "Batch [7/22]\n",
      "Batch [7/22], Generator Loss: 0.6358683109283447, Discriminator Loss: 1.4221951961517334\n",
      "Batch [8/22]\n",
      "Batch [8/22], Generator Loss: 0.6440486311912537, Discriminator Loss: 1.421299934387207\n",
      "Batch [9/22]\n",
      "Batch [9/22], Generator Loss: 0.6566146016120911, Discriminator Loss: 1.4129786491394043\n",
      "Batch [10/22]\n",
      "Batch [10/22], Generator Loss: 0.6731444597244263, Discriminator Loss: 1.40327787399292\n",
      "Batch [11/22]\n",
      "Batch [11/22], Generator Loss: 0.693960428237915, Discriminator Loss: 1.3939087390899658\n",
      "Batch [12/22]\n",
      "Batch [12/22], Generator Loss: 0.7168923020362854, Discriminator Loss: 1.3807930946350098\n",
      "Batch [13/22]\n",
      "Batch [13/22], Generator Loss: 0.7413526177406311, Discriminator Loss: 1.3674898147583008\n",
      "Batch [14/22]\n",
      "Batch [14/22], Generator Loss: 0.7671253681182861, Discriminator Loss: 1.3542914390563965\n",
      "Batch [15/22]\n",
      "Batch [15/22], Generator Loss: 0.7932859063148499, Discriminator Loss: 1.3368972539901733\n",
      "Batch [16/22]\n",
      "Batch [16/22], Generator Loss: 0.8192158937454224, Discriminator Loss: 1.3295363187789917\n",
      "Batch [17/22]\n",
      "Batch [17/22], Generator Loss: 0.8454934358596802, Discriminator Loss: 1.321972370147705\n",
      "Batch [18/22]\n",
      "Batch [18/22], Generator Loss: 0.8726950883865356, Discriminator Loss: 1.3121583461761475\n",
      "Batch [19/22]\n",
      "Batch [19/22], Generator Loss: 0.9015757441520691, Discriminator Loss: 1.2908544540405273\n",
      "Batch [20/22]\n",
      "Batch [20/22], Generator Loss: 0.9320457577705383, Discriminator Loss: 1.289975881576538\n",
      "Batch [21/22]\n",
      "Batch [21/22], Generator Loss: 0.9641675353050232, Discriminator Loss: 1.2811609506607056\n",
      "Batch [22/22]\n",
      "Batch [22/22], Generator Loss: 0.9981830716133118, Discriminator Loss: 1.2636098861694336\n",
      "Validation Loss: 0.9981873989105224\n",
      "Epoch [0/100], Generator Loss: 0.9981870651245117, Discriminator Loss: 1.2636098861694336, Validation Loss: 0.9981873989105224\n",
      "Epoch [1/100]\n",
      "Batch [1/22]\n",
      "Batch [1/22], Generator Loss: 1.033185601234436, Discriminator Loss: 1.2692744731903076\n",
      "Batch [2/22]\n",
      "Batch [2/22], Generator Loss: 1.069817304611206, Discriminator Loss: 1.2374706268310547\n",
      "Batch [3/22]\n",
      "Batch [3/22], Generator Loss: 1.1069363355636597, Discriminator Loss: 1.2396818399429321\n",
      "Batch [4/22]\n",
      "Batch [4/22], Generator Loss: 1.1456754207611084, Discriminator Loss: 1.2131681442260742\n",
      "Batch [5/22]\n",
      "Batch [5/22], Generator Loss: 1.1853110790252686, Discriminator Loss: 1.2014015913009644\n",
      "Batch [6/22]\n",
      "Batch [6/22], Generator Loss: 1.2272204160690308, Discriminator Loss: 1.1723098754882812\n",
      "Batch [7/22]\n",
      "Batch [7/22], Generator Loss: 1.2683041095733643, Discriminator Loss: 1.1757490634918213\n",
      "Batch [8/22]\n",
      "Batch [8/22], Generator Loss: 1.3093862533569336, Discriminator Loss: 1.159989356994629\n",
      "Batch [9/22]\n",
      "Batch [9/22], Generator Loss: 1.3501989841461182, Discriminator Loss: 1.1320083141326904\n",
      "Batch [10/22]\n",
      "Batch [10/22], Generator Loss: 1.384814739227295, Discriminator Loss: 1.1453914642333984\n",
      "Batch [11/22]\n",
      "Batch [11/22], Generator Loss: 1.416353702545166, Discriminator Loss: 1.1075923442840576\n",
      "Batch [12/22]\n",
      "Batch [12/22], Generator Loss: 1.4337505102157593, Discriminator Loss: 1.1497857570648193\n",
      "Batch [13/22]\n",
      "Batch [13/22], Generator Loss: 1.4432741403579712, Discriminator Loss: 1.1054468154907227\n",
      "Batch [14/22]\n",
      "Batch [14/22], Generator Loss: 1.441611409187317, Discriminator Loss: 1.106705904006958\n",
      "Batch [15/22]\n",
      "Batch [15/22], Generator Loss: 1.429522156715393, Discriminator Loss: 1.084967017173767\n",
      "Batch [16/22]\n",
      "Batch [16/22], Generator Loss: 1.4163634777069092, Discriminator Loss: 1.0398836135864258\n",
      "Batch [17/22]\n",
      "Batch [17/22], Generator Loss: 1.4018433094024658, Discriminator Loss: 1.0315032005310059\n",
      "Batch [18/22]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(\"DataLoaders created.\")\n",
    "\n",
    "# Models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(\"Models loaded.\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"Loss function loaded.\")\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "print(\"Optimizers created.\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Plot losses\n",
    "def plot_loss(d_losses, g_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Generator and Discriminator Loss\")\n",
    "    plt.plot(g_losses, label=\"G\")\n",
    "    plt.plot(d_losses, label=\"D\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "\n",
    "    for i, (low_res, high_res) in enumerate(train_dataloader):\n",
    "        print(f\"Batch [{i+1}/{len(train_dataloader)}]\")\n",
    "\n",
    "        # Move data to device\n",
    "        low_res = low_res.to(device)\n",
    "        high_res = high_res.to(device)\n",
    "\n",
    "        ##############################\n",
    "        # Train the discriminator\n",
    "        ##############################\n",
    "\n",
    "        # Forward pass real samples through discriminator\n",
    "        real_outputs = discriminator(high_res)\n",
    "        real_labels = torch.ones_like(real_outputs).to(device)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "\n",
    "        # Generate fake images and forward pass through discriminator\n",
    "        generated_imgs = generator(low_res)\n",
    "        fake_outputs = discriminator(generated_imgs.detach())\n",
    "        fake_labels = torch.zeros_like(fake_outputs).to(device)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "        # Compute discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        ##############################\n",
    "        # Train the generator\n",
    "        ##############################\n",
    "\n",
    "        # Forward pass fake images through discriminator\n",
    "        fake_outputs = discriminator(generated_imgs)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Save losses for plotting\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "        print(f\"Batch [{i+1}/{len(train_dataloader)}], Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        num_val_samples = 0\n",
    "\n",
    "        for i, (low_res, high_res) in enumerate(val_dataloader):\n",
    "            low_res = low_res.to(device)\n",
    "            high_res = high_res.to(device)\n",
    "\n",
    "            batch_size = low_res.size(0)  # Get the actual batch size\n",
    "\n",
    "            generated_imgs = generator(low_res[:batch_size])\n",
    "            fake_outputs = discriminator(generated_imgs)\n",
    "            real_labels = torch.ones_like(fake_outputs).to(device)  # Adjust real_labels to match the batch size\n",
    "            g_loss = criterion(fake_outputs, real_labels[:batch_size])\n",
    "            val_loss += g_loss.item() * batch_size\n",
    "            num_val_samples += batch_size\n",
    "\n",
    "        val_loss /= num_val_samples\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}, Validation Loss: {val_loss}\")\n",
    "\n",
    "# Plot losses\n",
    "plot_loss(d_losses, g_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5nPubmXvslT"
   },
   "outputs": [],
   "source": [
    "# Validation of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJnFPINsvoLf"
   },
   "outputs": [],
   "source": [
    "# Evaluate the network on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Print GPU device information\n",
    "    for gpu in range(num_gpus):\n",
    "        print(f\"GPU {gpu}: {torch.cuda.get_device_name(gpu)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!py -m pip uninstall torch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
